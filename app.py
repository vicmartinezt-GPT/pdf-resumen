# ========= HOTFIX: mostrar cualquier error y a√±adir diagn√≥stico =========
import streamlit as st
st.set_page_config(page_title="PDF -> Resumen configurable", page_icon="üìÑ", layout="wide")

try:
    import os, io
    from typing import List, Dict, Any
    from dotenv import load_dotenv
    from pypdf import PdfReader
    from openai import OpenAI
    load_dotenv()
except Exception as e:
    st.error("‚ùå Fallo al importar dependencias. Revisa requirements.txt o conexi√≥n.")
    st.exception(e)
    st.stop()

# ============================= Funciones auxiliares =============================
def extract_text_from_pdf(data: bytes) -> str:
    """Extrae texto de un PDF en memoria (bytes)."""
    reader = PdfReader(io.BytesIO(data))
    pages_text = []
    for page in reader.pages:
        try:
            pages_text.append(page.extract_text() or "")
        except Exception:
            pages_text.append("")
    return "\n\n".join(pages_text)

def chunk_text(text: str, max_chars: int = 7000, overlap: int = 500) -> List[str]:
    """Divide el texto en trozos con solapamiento."""
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + max_chars, n)
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

# ============================= Config de resumen =============================
DEFAULT_OBJECTIVE = (
    "Generar un resumen ejecutivo claro y estructurado del documento, "
    "enfocado en decisiones y hallazgos clave."
)
DEFAULT_SECTIONS = [
    {"id": "titulo", "label": "T√≠tulo Principal",
     "instruction": "Encabeza con la acci√≥n principal o idea-fuerza del documento.", "required": True},
    {"id": "contexto", "label": "Contexto",
     "instruction": "Explica brevemente el contexto, alcance y prop√≥sito.", "required": True},
    {"id": "hallazgos", "label": "Hallazgos/Conclusiones",
     "instruction": "Enumera los hallazgos clave y sus implicancias.", "required": True},
    {"id": "afectacion", "label": "Afectaci√≥n/Impacto (omitir si no aplica)",
     "instruction": "Solo incluir si el documento contiene datos claros de afectaci√≥n/impacto; si no hay evidencia, omitir por completo esta secci√≥n.",
     "required": False, "omit_if_empty": True},
    {"id": "recomendaciones", "label": "Recomendaciones/Cursos de acci√≥n",
     "instruction": "Lista recomendaciones accionables priorizadas (si existen).", "required": False},
    {"id": "fuentes", "label": "Fuentes/Referencias (opcional)",
     "instruction": "Cita brevemente secciones/p√°ginas relevantes del PDF.", "required": False},
]

AGGREGATION_SYSTEM_PROMPT = (
    "Eres un analista experto en s√≠ntesis documental. Recibes res√∫menes parciales "
    "(por trozos) y debes consolidarlos en un √∫nico resumen final coherente, sin "
    "repeticiones, manteniendo trazabilidad ligera a p√°ginas cuando sea posible."
)
CHUNK_SYSTEM_PROMPT = (
    "Eres un analista experto. Resume el contenido del trozo dado en relaci√≥n con el objetivo y las secciones definidas. "
    "Respeta las reglas de omitir secciones cuando no haya evidencia. No inventes informaci√≥n."
)

# ============================= Funciones LLM =============================
def call_llm(client: OpenAI, model: str, system_prompt: str, user_prompt: str, temperature: float = 0.2) -> str:
    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": system_prompt},
                  {"role": "user", "content": user_prompt}],
        temperature=temperature,
    )
    return resp.choices[0].message.content.strip()

def build_chunk_user_prompt(objetivo: str, secciones, chunk: str, idx: int, total: int) -> str:
    template = [f"OBJETIVO: {objetivo}", "SECCIONES (con reglas):"]
    for s in secciones:
        rule = " [OMITIR SI VAC√çO]" if s.get("omit_if_empty") else (" [REQUERIDA]" if s.get("required") else "")
        template.append(f"- {s['label']}: {s['instruction']}{rule}")
    template.append("")
    template.append(f"TROZO {idx+1}/{total} (no inventes, no extrapoles sin evidencia):")
    template.append(chunk)
    template.append("")
    template.append("DEVUELVE UN JSON con claves igual al id de cada secci√≥n y valores textuales limpios. \n"
                    "Incluye solo secciones con contenido; si una secci√≥n no aplica o est√° vac√≠a, om√≠tela del JSON.")
    return "\n".join(template)

def build_aggregation_user_prompt(objetivo: str, secciones, partial_json_list) -> str:
    template = [f"OBJETIVO: {objetivo}", "SECCIONES TOTALES (estructura final):"]
    for s in secciones:
        rule = " [OMITIR SI VAC√çO]" if s.get("omit_if_empty") else (" [REQUERIDA]" if s.get("required") else "")
        template.append(f"- {s['id']}: {s['label']} ‚Äî {rule}")
    template.append("")
    template.append("A continuaci√≥n tienes una lista de res√∫menes parciales en formato JSON (uno por trozo). \n"
                    "Fusi√≥nalos en un √∫nico resumen final coherente, sin repeticiones, siguiendo el orden de secciones.\n"
                    "No inventes informaci√≥n no presente en los parciales.")
    template.append("\nRES√öMENES PARCIALES:\n" + "\n\n".join(partial_json_list))
    template.append("")
    template.append("DEVUELVE SOLO UN JSON FINAL con las claves de secci√≥n y valores textuales (Markdown permitido).")
    return "\n".join(template)

# ============================= UI =============================
st.title("üìÑ PDF ‚Üí Resumen configurable")

with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    # Fallback a modelos alternativos si el predeterminado no est√° habilitado en tu cuenta
    model = st.text_input("Modelo (chat.completions)", value=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.05)
    max_chars = st.number_input("Tama√±o de trozo (chars)", min_value=2000, max_value=12000, value=7000, step=500)
    overlap = st.number_input("Solapamiento (chars)", min_value=0, max_value=2000, value=500, step=50)

    st.divider()
    st.caption("Define objetivo y secciones del resumen")
    objetivo = st.text_area("Objetivo del resumen", value=DEFAULT_OBJECTIVE, height=80)

    st.subheader("Secciones")
    edited_sections = []
    for s in DEFAULT_SECTIONS:
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            label = st.text_input(f"Etiqueta ‚Äî {s['id']}", value=s["label"], key=f"label_{s['id']}")
            instruction = st.text_area(f"Instrucci√≥n ‚Äî {s['id']}", value=s["instruction"], key=f"instr_{s['id']}")
        with col2:
            required = st.checkbox("Requerida", value=s.get("required", False), key=f"req_{s['id']}")
        with col3:
            omit_if_empty = st.checkbox("Omitir si vac√≠o", value=s.get("omit_if_empty", False), key=f"omit_{s['id']}")
        edited_sections.append({
            "id": s["id"], "label": label, "instruction": instruction,
            "required": required, "omit_if_empty": omit_if_empty,
        })

    st.divider()
    st.caption("Clave API")
    api_key = st.secrets.get("OPENAI_API_KEY", "") or st.text_input(
        "OPENAI_API_KEY", type="password", value=os.getenv("OPENAI_API_KEY", "")
    )

# ---------- Panel de diagn√≥stico ----------
with st.expander("üîé Diagn√≥stico r√°pido", expanded=False):
    st.write("**Variables detectadas:**")
    st.write({
        "api_key_configurada": bool(api_key),
        "longitud_api_key": len(api_key) if api_key else 0,
        "modelo": model,
    })
    if st.button("üß™ Probar conexi√≥n OpenAI"):
        try:
            if not api_key:
                st.error("No hay OPENAI_API_KEY configurada.")
            else:
                import os
                os.environ["OPENAI_API_KEY"] = api_key
                client = OpenAI()
                test = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": "Di 'ok' si me recibes"}],
                    temperature=0.0,
                )
                st.success("Conexi√≥n OK ‚úÖ")
                st.code(test.choices[0].message.content, language="text")
        except Exception as e:
            st.error("‚ùå Fall√≥ la prueba de conexi√≥n. Revisa el mensaje:")
            st.exception(e)

# ============================= Flujo principal =============================
uploaded = st.file_uploader("Sube un PDF", type=["pdf"])

if uploaded is not None and api_key:
    try:
        data = uploaded.getvalue()
        reader = PdfReader(io.BytesIO(data))
        text = extract_text_from_pdf(data)
        st.info(f"P√°ginas detectadas: {len(reader.pages)} | Longitud texto: {len(text)} chars")

        if st.button("‚ñ∂Ô∏è Generar resumen"):
            with st.spinner("Procesando‚Ä¶"):
                chunks = chunk_text(text, max_chars=max_chars, overlap=overlap)
                import os
                os.environ["OPENAI_API_KEY"] = api_key
                client = OpenAI()

                partials = []
                for i, ch in enumerate(chunks):
                    uprompt = build_chunk_user_prompt(objetivo, edited_sections, ch, i, len(chunks))
                    out = call_llm(client, model, CHUNK_SYSTEM_PROMPT, uprompt)
                    partials.append(out)
                    st.write(f"Trozo {i+1}/{len(chunks)} procesado.")

                agg_prompt = build_aggregation_user_prompt(objetivo, edited_sections, partials)
                final_json = call_llm(client, model, AGGREGATION_SYSTEM_PROMPT, agg_prompt)

                st.subheader("Resumen final (JSON)")
                st.code(final_json, language="json")
                st.download_button("üíæ Descargar JSON", "resumen.json", "application/json", final_json.encode("utf-8"))

    except Exception as e:
        st.error("‚ùå Error durante el procesamiento del PDF o la llamada al modelo.")
        st.exception(e)
elif uploaded is None:
    st.warning("Sube un PDF para comenzar.")
elif not api_key:
    st.warning("Configura tu OPENAI_API_KEY en Secrets o en la barra lateral.")
